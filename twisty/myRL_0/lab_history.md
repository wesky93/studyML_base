# 실험 기록 관리
여기에 실험을 할때마다 변경사항과 실험로그 이름을 기록해둔다.

# 양식

---
## lab_날짜
### 게임 규칙 변경사항
- 기본 게임규칙 적용
- 기본 보상 = 0
- 최종 완성 보상 = 100

### 모델 변경사항
-

### 결과
-

### 기타
-
---

# 기록

---
## lab1_20170212
### 게임 규칙 변경사항
- 기본 보상 = 0
- 최종 완성 보상 = 100

### 모델 변경사항
- cost : reduce_mean -> reduce_sum 으로 변경

### 결과
- 제대로 학습 않됨

### 기타
- 최소 학습 방식을 바꿔서 다시 시도해보자 1000번만으로는 제대로 학습안될 가능성이 높음
---


---
## lab2_20170214
### 게임 규칙 변경사항
- 이전과 동일

### 모델 변경사항
- 최소 학습 횟수를 1000 -> 100000 으로 변경
- 랜덤확률이 가감되는 크기를 동적으로 변경하도록 함

### 결과
- 초반에 랜덤값을 많이 잡아주니 확실히 이후 cost가 lab1대비 많이 낮춰짐
- 하지만 최종 점수에는 아직 큰 차이가 없음

### 기타
- 일단 최종 점수도 텐서보드에서 볼수 있도록 방법을 강구해봐야 할듯함
-
---

---
## lab3_20170216
### 게임 규칙 변경사항
- 이전과 동일

### 모델 변경사항
- 보상,성공확률,회전횟수를 텐서보드에 기록하도록 함
- 모델에 텐서보드를 위한 그룹 작업을 해
- 감마 값을 0.99 -> 0.5로 변경

### 결과
- cost는 낮아졌으나 보상은 차도가 없다.

### 기타
- 더이상 학습은 무의미 해보이므로 일단 스크램 횟수를 5회로 낮추어 실험을 해야 할듯 하다.
---

---
## lab4_20170216
### 게임 규칙 변경사항
- 스크램크기를 10 -> 5 로 변경

### 모델 변경사항
- 감마 값을 다시 원상복귀(0.5 -> 0.99)


### 결과
- 게임이 쉬워지니 조금더 쉽게 보상을 얻으며 확실히 보상이 상승하는것을 확인함
- 다만 최종 완성으로 가기 보다 중간 보상만을 취득하여 고득점을 달성하려는 문제가 있음

### 기타
- 해결방안은 2가지 일듯 하다. 하나는 보상 방식을 최종 단계에서만 습득하게 하거나 좀더 장기간의 결과로 점수를 얻게 하는 것이다.
- 일단 중간 보상의 부여하는것을 유지하되 규칙 혹은 모델을 변경함으로써 추가적인 학습이 이뤄지게 해야 할듯 싶다.
- 다음 실험을 하기 전에 먼저 메모리 학습을 구현한뒤 실험을 하는 것이 좋을듯 싶다. 어찌됬던 랜덤한 학습이 결과에 좋은 영향을 미치는것은 사실인것 같으니.
---

---
## lab5_20170217
### 게임 규칙 변경사항
- 게임 상태를 내보낼때 상태의 shpae을 (1,6,8)에서 (6,8)로 변경함
    메모리에 게임 진행상황을 저장한뒤 나중에 샘플링 자료를 취합하기에 상태 그대로 저장하게 바꾼것

### 모델 변경사항
- 게임 진행 상황을 메모리에 저장한뒤 이 자료를 샘플링한것으로 학습 하도록 변경함(DQN구현)
    총 50,000개의 기록을 저장하며 여기서 50개를 샘플링하여 학습
- 게임 오버될경우 reward_y값에 다음 Q값을 더하지 않고 현재 보상만 값만 부하도록 함
- 텐서보드에 기록하는 평균 전체보상, 완성 확률, 평균 전체 회전수의 기록 단위를 테스트 진행 횟수 에서 게임 진행 횟수로 변경
    이에따라 이전 기록과 단순비교하기 힘듦, lab5는 이전 기록과 그래프상에서 비교하면 안

### 결과
- lab4이전의 실험보다는 평균점수가 높으나 이는 난이도가 쉬워서(스크램블 사이즈가 5임)그런것으로 실질적인 효과는 없다고 봐도 무방할듯 하다.

### 기타
- 현재 생각되는 문제는 중간 학습 단계에서 더이상 발전이 없다는 것이다.
    그래서 생각한 부분이 우리는 보통 무언가를 학습할때 쉬운것에서 어려운것으로 단계적으로 학습한다.
    즉, 스크램 3번정도에서 학습을 하고 해당 가중치를 스크램 5에서 학습 이후 단계적으로 10까지 학습하면 되지 않을까 싶다.
    대신 이렇게 실험 할경우 게임규칙(스크램사이즈)만 변경되어야 하며 절대 다른 규칙이 변경되어서는 안된다.
- 일단 relu를 제거하여 학습 결과를 살펴 보고 문제가 없다 싶으면 단계별 학습을 실험 해보면 좋을것 같다.
   
---

## lab6_20170219
### 목표
- 최하 난이도에서 학습을 한뒤 이 학습자료를 이용해 점점 난이도를 높여가며 학습한다.

### 게임규칙 변경사항
- 난이도를 낮춰 스크램블 길이는 3이고 스크램이 제대로 섞였는지 확인하지 않도록 한다.

### 모델 변경사항
- 학습을 종료할때 매번 학습내용이 저장되도록 변경
- 또한 이전 학습을 불러 올 수 있도록 함

### 결과
-

### 기타
- 게임이 완성되었을 경우 완성되었다고 return되지 않으며 보상이 잘못 주어지는 심각한 버그를 발견함에 따라
    이전 실험 자료는 모두 잘못될 수 밖에 없음, 버그 수정후 실질적으로 실험을 처음부터 시작해야함

---

---

## lab7_20170219
### 목표
- 보상 문제를 해결한 이후 아주 쉬운 난이도에서 문제 해결하기

### 게임규칙 변경사항
- 이전 모든 실험에서 보상이 잘못 매겨지는 심각한 버그를 수정함

### 모델 변경사항
- 변경사항 없음

### 결과
- 평균 보상, 평균 회전수 모두 성공적으로 학습 되었음을 확인 할 수 있다.
- 다만, 스크램길이가 3인것에 비하여 평균 회전수 값이 큰편인것으로 생각됨

### 기타
- 전반적인 값들을 늘려서 실험을 해보고 좀더 나은 것을 기반으로 이후 단계별 학습을 진행 하면 될듯 싶다.

---
---

## lab8_20170220
### 목표
- 좀더 정밀한 뉴런으로 보다 고득점을 얻기

### 게임규칙 변경사항
- 한게임당 최대 회전수를 50에서 100으로 변경

### 모델 변경사항
- 1,2,3 히든레이어의 뉴런갯수 10배 증가, 풀커넥티드레이어의 뉴런갯수 2배 증가

### 결과
- 시간으로 봤을때 이전과 유사한 결과가 나오지만 매 배치간 학습이 너무 길다는 문제가 있음

### 기타
- 뉴런 갯수를 이전 10배에서 4배로 바꿔 다시 시도해보기
---

---

## lab8_20170220
### 목표
- 좀더 정밀한 뉴런으로 보다 고득점을 얻기

### 게임규칙 변경사항
- 한게임당 최대 회전수를 50에서 100으로 변경

### 모델 변경사항
- 1,2,3 히든레이어의 뉴런갯수 10배 증가, 풀커넥티드레이어의 뉴런갯수 2배 증가

### 결과
- 학습은 되지만 lab7에 비해 보상점수가 상대적으로 높다

### 기타
- lab7과 lab8의 중간정도 값으로 실험을 해보고 이를 lab7과 비교하면 좋을듯 싶다

---

---

## lab9_20170220
### 목표
- lab7,8을 결과를 활용한 트레이드오프 테스트

### 게임규칙 변경사항
- lab8과 동일

### 모델 변경사항
- 히든레이어의 뉴런갯수를 lab7에 비해 4배 증가, fc갯수는 lab8과 동일

### 결과
- 학습은 되지만 가시적인 효과는 보이지 않으며 일정 시간이 지나면 더이상 보상값이 효과적으로 오르지 않는다.

### 기타
- 게임을 100번 진행한다 해도 회전횟수가 빨리 줄어 들지 않으므로 회전 횟수는 50번으로 변경하면 좋을듯 싶다. 

---

---

## lab10_20170220
### 목표
- lab9의 모델을 기반으로 lab7의 게임규칙을 이용한 실험을 통해 lab7과 lab9을 비교

### 게임규칙 변경사항
- 게임 규칙(게임당 최대 회전수)은 lab7를 적용

### 모델 변경사항
- 각 레이어의 뉴런갯수는 lab9과 동일(144,288,576,2048)

### 결과
- 초반에는 잘 학습하는듯 하다 20시간이 넘는 시점부터는 평균회전이 올라가서 제대로 학습이 안되는 상황이 발생함
- 뉴런 갯수가 많아도 제대로 학습이 되지 않음을 확인 했기에 lab7과 비슷한 상황에서 다시 학습한뒤 난이도를 올리는 방식을 시작하면 될듯함

### 기타
- per_done을 계산할때 사용하는 변수명이 잘못되어 여태 잘못된 결과가 나온것을 확인함
    즉시 수정했으며 lab11부터는 완성확률의 경우 이전자료와 비교를 하면 안됨
- 완성된 큐브만의 평균 회전수만 따로 확인할 필요가 있어보며 해당부분을 기록하는 기능을 추가해야함

---

---

## lab11_20170221
### 목표
- lab7과 유사한 상황에서 재학습(per_done을 고친뒤 다시 자료를 습득할 필요가 있음)

### 게임규칙 변경사항
- lab7과 동일

### 모델 변경사항
- lab7과 동일

### 결과
- 

### 기타
- 

---

