# 실험 기록 관리
여기에 실험을 할때마다 변경사항과 실험로그 이름을 기록해둔다.

# 양식

---
## lab_날짜
### 게임 규칙 변경사항
- 기본 게임규칙 적용
- 기본 보상 = 0
- 최종 완성 보상 = 100

### 모델 변경사항
-

### 결과
-

### 기타
-
---

# 기록

---
## lab1_20170212
### 게임 규칙 변경사항
- 기본 보상 = 0
- 최종 완성 보상 = 100

### 모델 변경사항
- cost : reduce_mean -> reduce_sum 으로 변경

### 결과
- 제대로 학습 않됨

### 기타
- 최소 학습 방식을 바꿔서 다시 시도해보자 1000번만으로는 제대로 학습안될 가능성이 높음
---


---
## lab2_20170214
### 게임 규칙 변경사항
- 이전과 동일

### 모델 변경사항
- 최소 학습 횟수를 1000 -> 100000 으로 변경
- 랜덤확률이 가감되는 크기를 동적으로 변경하도록 함

### 결과
- 초반에 랜덤값을 많이 잡아주니 확실히 이후 cost가 lab1대비 많이 낮춰짐
- 하지만 최종 점수에는 아직 큰 차이가 없음

### 기타
- 일단 최종 점수도 텐서보드에서 볼수 있도록 방법을 강구해봐야 할듯함
-
---

---
## lab3_20170216
### 게임 규칙 변경사항
- 이전과 동일

### 모델 변경사항
- 보상,성공확률,회전횟수를 텐서보드에 기록하도록 함
- 모델에 텐서보드를 위한 그룹 작업을 해
- 감마 값을 0.99 -> 0.5로 변경

### 결과
- cost는 낮아졌으나 보상은 차도가 없다.

### 기타
- 더이상 학습은 무의미 해보이므로 일단 스크램 횟수를 5회로 낮추어 실험을 해야 할듯 하다.
---

---
## lab4_20170216
### 게임 규칙 변경사항
- 스크램크기를 10 -> 5 로 변경

### 모델 변경사항
- 감마 값을 다시 원상복귀(0.5 -> 0.99)


### 결과
- 게임이 쉬워지니 조금더 쉽게 보상을 얻으며 확실히 보상이 상승하는것을 확인함
- 다만 최종 완성으로 가기 보다 중간 보상만을 취득하여 고득점을 달성하려는 문제가 있음

### 기타
- 해결방안은 2가지 일듯 하다. 하나는 보상 방식을 최종 단계에서만 습득하게 하거나 좀더 장기간의 결과로 점수를 얻게 하는 것이다.
- 일단 중간 보상의 부여하는것을 유지하되 규칙 혹은 모델을 변경함으로써 추가적인 학습이 이뤄지게 해야 할듯 싶다.
- 다음 실험을 하기 전에 먼저 메모리 학습을 구현한뒤 실험을 하는 것이 좋을듯 싶다. 어찌됬던 랜덤한 학습이 결과에 좋은 영향을 미치는것은 사실인것 같으니.
---

---
## lab5_20170217
### 게임 규칙 변경사항
- 게임 상태를 내보낼때 상태의 shpae을 (1,6,8)에서 (6,8)로 변경함
    메모리에 게임 진행상황을 저장한뒤 나중에 샘플링 자료를 취합하기에 상태 그대로 저장하게 바꾼것

### 모델 변경사항
- 게임 진행 상황을 메모리에 저장한뒤 이 자료를 샘플링한것으로 학습 하도록 변경함(DQN구현)
    총 50,000개의 기록을 저장하며 여기서 50개를 샘플링하여 학습
- 게임 오버될경우 reward_y값에 다음 Q값을 더하지 않고 현재 보상만 값만 부하도록 함
- 텐서보드에 기록하는 평균 전체보상, 완성 확률, 평균 전체 회전수의 기록 단위를 테스트 진행 횟수 에서 게임 진행 횟수로 변경
    이에따라 이전 기록과 단순비교하기 힘듦, lab5는 이전 기록과 그래프상에서 비교하면 안

### 결과
- lab4이전의 실험보다는 평균점수가 높으나 이는 난이도가 쉬워서(스크램블 사이즈가 5임)그런것으로 실질적인 효과는 없다고 봐도 무방할듯 하다.

### 기타
- 현재 생각되는 문제는 중간 학습 단계에서 더이상 발전이 없다는 것이다.
    그래서 생각한 부분이 우리는 보통 무언가를 학습할때 쉬운것에서 어려운것으로 단계적으로 학습한다.
    즉, 스크램 3번정도에서 학습을 하고 해당 가중치를 스크램 5에서 학습 이후 단계적으로 10까지 학습하면 되지 않을까 싶다.
    대신 이렇게 실험 할경우 게임규칙(스크램사이즈)만 변경되어야 하며 절대 다른 규칙이 변경되어서는 안된다.
- 일단 relu를 제거하여 학습 결과를 살펴 보고 문제가 없다 싶으면 단계별 학습을 실험 해보면 좋을것 같다.
   
---